---
title: "湖仓一体数据架构介绍"
categories: [05_DataEngineering]
tags: [大数据]

layout: post
---

湖仓一体（Lakehouse）是一种数据架构：在低成本数据湖存储之上，引入数仓级表管理、事务、一致性和性能能力，使同一份数据既能支撑离线数仓建模，又能支撑交互式分析与机器学习。

---

## 基本概念

1. **统一存储**：数据只存一份，存储在对象存储（S3、OSS、HDFS-Object 等）
2. **统一表模型**：通过 Iceberg、Hudi、Delta Lake 等表格式提供 ACID、Schema 演进、快照管理
3. **统一元数据**：表结构、分区、文件路径等元数据集中管理，支持快速查询和多引擎访问
4. **多引擎计算**：同一数据可以被 Spark、Flink、Trino、Hive 等多种计算引擎共享

## 架构剖析

**传统数据仓库（数仓）**

* 存储在专用硬件 / 高性能 SSD
* 数据分层：ODS → DWD → DWS → DM
* 优点：治理能力强、查询性能高
* 缺点：

  * 存储成本高，扩展慢
  * 小文件、分区爆炸导致元数据访问慢
  * Schema 演进困难
  * 流批处理分离

**数据湖**

* 存储在廉价对象存储
* 格式自由：Parquet、ORC、CSV、JSON
* 优点：低成本、无限扩展、支持多类型数据
* 缺点：

  * 无事务支持，无法安全更新/删除
  * 缺乏表级治理
  * 查询性能不稳定

**湖仓一体架构**

**将数据湖的低成本存储和数仓的表管理、事务能力结合，通过Iceberg/Delta/Hudi 表格式，实现“湖+ 仓”的统一平台。**

一、核心组件

| 组件                                    | 作用                       |
| ------------------------------------- | ------------------------ |
| 对象存储（S3/OSS/HDFS-Object）              | 数据物理存储，低成本、高扩展性          |
| 表格式（Iceberg/Hudi/Delta）               | ACID、Schema 演进、快照管理、增量更新 |
| 元数据管理（Hive Metastore / Nessie / Glue） | 统一管理表结构、分区、文件路径等信息       |
| 多计算引擎（Spark/Flink/Trino/Hive）         | 批处理、流处理、交互式分析、ML         |

二、数据访问流程（以 Iceberg 为例）

* 读取最新 Snapshot 获取 Manifest 文件
* 从 Manifest 文件获取 Data File 列表及分区信息
* Partition Pruning / File-level Filtering
* 构建物理执行计划并执行查询

> 查询启动时间几乎不受分区数量影响，支持高并发和小文件治理。

三、数据分层仍然保留

虽然底层存储在对象存储，但逻辑分层（ODS/DWD/DWS/DM）依然重要：

* 保证数据质量和血缘管理
* 方便团队协作
* 面向 BI、报表和 ML 提供清晰的数据接口

---

**湖仓一体优势**

| 优势              | 说明                                                     |
| --------------- | ------------------------------------------------------ |
| 存储成本低           | 对象存储 + 压缩列式文件（Parquet/ORC）                             |
| 扩展性强            | PB 级数据，按需扩容，无需改集群                                      |
| 查询性能好           | Manifest 快照 + Partition Pruning + File-level Filtering |
| 支持增量/CDC/Upsert | Iceberg/Hudi/Delta 提供 ACID                             |
| Schema 演进安全     | 新增字段、删除字段对历史数据透明                                       |
| 多引擎共享           | 同一表可被 Spark、Flink、Trino、Hive 查询                        |

---

## 数据访问方式

**传统数仓的元数据访问瓶颈**

一、分区数量巨大

* Hive 传统建表习惯：`PARTITIONED BY dt` 或多级分区
* 当分区达到几十万、上百万时：
  * Hive 需要向 Metastore 查询所有分区信息
  * 生成查询计划时，要枚举每个分区的路径
* 查询启动时间 = **Metastore 查询 + FileStatus 扫描**

某电商日志表的实际情况：
* 每天一个分区 → 3 年 ~ 1000 天 → 1000 分区
* 每天每小时分区 → 1000×24 = 24000 分区
* Hive 查询启动可能耗时 **几十秒到几分钟**

---

二、分区爆炸 + small file

* 每个分区有几十个小文件
* Hive 查询需要：
  1. 查询 Metastore 拿到分区列表
  2. 列出每个分区下的文件（HDFS `listStatus`）
* 这个 I/O 成本和 Namenode 交互次数成正比

---

三、查询计划生成消耗 CPU / 内存

* Hive 需要把所有分区、文件信息读进内存
* 大表 + 大量分区 → 查询计划膨胀
* 甚至可能导致 Driver OOM

---

**Iceberg 的核心在于 表格式 + Snapshot + Manifest，彻底改变元数据访问方式。**

一、表格式（Iceberg Table）

* 表不是依赖目录结构，而是 **逻辑表 + Manifest 文件 + Data 文件**
* **Manifest** 是一个 JSON/Avro 文件，记录：
  * 分区值
  * 数据文件路径
  * 文件行数、大小
  * 列的 min/max 值（用于列裁剪 / 数据跳过）

```
Iceberg Table
├── snapshots/
│   ├── snap-1.json
│   └── snap-2.json
├── manifests/
│   ├── manifest-1.avro
│   └── manifest-2.avro
└── data/
    ├── part-0001.parquet
    └── part-0002.parquet
```

---

二、Snapshot 原理

* 每次写操作（Append / Overwrite / Delete）生成 **新的 Snapshot**
* Snapshot 记录 **当前表的 Manifest 列表**
* 查询时只需读取最新 Snapshot → Manifest → Data File
* 查询时不需要再动态扫描目录，**查询成本和分区数量无关**

---

三、查询计划生成流程（Iceberg）

以 `SELECT * FROM big_table WHERE dt='20260107'` 为例：

1. 读取最新 Snapshot → 获取 Manifest 文件列表
2. 读取 Manifest 文件 → 得到 Data File 列表和分区值、列 min/max
3. 扫描文件时可以：

   * **Partition Pruning**：直接选中 dt='20260107' 的文件
   * **File-level Filtering**：用 min/max skip 不相关文件
4. 构建 Physical Plan → Spark / Trino Job

关键点：

* 查询启动时间 ≈ O(Manifest 文件大小)
* 与表的分区数量无关
* HDFS / 对象存储只读一次 Manifest 文件，无需 listStatus 每个目录

---

四、小文件治理与元数据优势

* Manifest 文件记录数据文件，不依赖目录层级
* 即使小文件很多，查询也只扫描 Manifest → 数据文件，不用在 Namenode 做大量 RPC
* 定期 **Rewrite / Compaction** 可以减少小文件，但不会影响查询启动

---

**对比总结（底层原理版）**

| 维度        | 传统 Hive              | Iceberg / Lakehouse     |
| --------- | -------------------- | ----------------------- |
| 元数据存储     | HMS + HDFS 目录        | Snapshot + Manifest     |
| 分区访问      | 查询时枚举所有目录            | 读取 Manifest 文件，直接定位文件   |
| 文件访问      | HDFS listStatus 每个分区 | Manifest 中记录文件路径，减少 RPC |
| 查询启动时间    | O(分区数量 × 文件数量)       | O(Manifest 文件大小) ≈ 固定   |
| 小文件影响     | 显著                   | 不影响查询启动，易治理             |
| Schema 演进 | 麻烦                   | 表格式支持安全演进               |

---

## 疑难问题

> 为啥传统数仓的存储成本高呢？

传统数仓存储成本高，不是因为磁盘贵，而是整体系统设计为了性能、高可用和管理便利而带来的附加成本：专用硬件、数据副本、索引/物化视图、ETL 临时表，以及低磁盘利用率。
而数据湖/ 湖仓一体使用廉价对象存储+ 按需压缩+ 表格式管理，整体成本低很多。

| 特性        | 传统数仓表               | 湖仓一体表                       |
| --------- | ------------------- | --------------------------- |
| 存储        | 专用存储（昂贵，固定）         | 对象存储（廉价，可扩展）                |
| 文件格式      | ORC / Parquet，直接写文件 | Iceberg / Delta / Hudi 表格式  |
| 事务        | 受限                  | 完整 ACID 支持，增量安全写入           |
| 小文件       | 爆炸容易                | 可治理，支持 compaction / rewrite |
| Schema 变化 | 升级难                 | Schema 演进安全                 |


> 传统数仓+存算分离是否就可以媲美湖仓一体了呢？

Hive+对象存储可以解决存储成本问题，但不是完整的湖仓一体，尤其在小文件治理、事务、一致性、多引擎写入能力上仍然有限。

| 维度         | Hive+OBS | Lakehouse (Iceberg/Hudi) |
| ---------- | -------- | ------------------------ |
| 存储成本       | ✅ 低      | ✅ 低                      |
| 扩展性        | ✅ 好      | ✅ 好                      |
| 多引擎访问      | ✅ 可行     | ✅ 可行                     |
| 小文件管理      | ❌ 无      | ✅ 支持 compaction          |
| 并发写入       | ❌ 弱      | ✅ 强（乐观锁/事务）              |
| Schema 演进  | ❌ 弱      | ✅ 安全                     |
| CDC/Upsert | ❌ 弱      | ✅ 支持                     |


## 总结
湖仓一体并不是放弃数仓，而是通过 Iceberg 等表格式，把数据湖升级为具备数仓能力的统一数据平台。
在这一架构中，数据只存一份，但可以被多种计算引擎以一致、可回溯、可演进的方式使用，既解决了传统数仓的扩展和成本问题，也弥补了数据湖缺乏事务和治理能力的短板。

